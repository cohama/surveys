|回|ジャンル|github account（敬称略）|link|title||
|:---|:---|:---|:---|:---|:---|
|1|NN一般|takagi|https://arxiv.org/abs/1710.09829|Dynamic Routing Between Capsules|capsule net|
||強化学習|@kmiwa|https://arxiv.org/abs/1803.04675|Using Grouped Linear Prediction and Accelerated Reinforcement Learning for Online Content Caching||
||自然言語 チャットボット|@nharu1san|https://arxiv.org/abs/1612.01627|Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots||
||教育|@kaaztech|https://dl.acm.org/doi/10.1145/3027385.3029479|A neural network approach for students' performance prediction||
||NN一般 活性化|@antimon2|https://arxiv.org/abs/1710.05941|Searching for Activation Functions|swish|
||アノテーション 医療|@exoego|https://arxiv.org/abs/1708.06297|Employing Weak Annotations for Medical Image Analysis Problems||
||量子化|@cohama|https://arxiv.org/abs/1511.00363|BinaryConnect: Training Deep Neural Networks with binary weights during propagations||
||正規化|@melleo1978|https://arxiv.org/abs/1705.08741|Train longer, generalize better: closing the generalization gap in large batch training of neural networks|ghost batch normalization|
||ドメイン適用|@kotamatui|http://papers.nips.cc/paper/6963-joint-distribution-optimal-transportation-for-domain-adaptation.pdf|Joint distribution optimal transportation for domain adaptation||
||物体検出 zero_shot|@n-kats|https://arxiv.org/abs/1803.06049|Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concept||
|2|自然言語|@nharu1san|https://arxiv.org/abs/1802.02614v1|Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus||
||敵対的事例|@antimon2|https://arxiv.org/abs/1804.00499|Semantic Adversarial Examples||
||強化学習|@kmiwa|https://arxiv.org/abs/1603.00748|Continuous Deep Q-Learning with Model-based Acceleration||
||物体検出|@n-kats|https://arxiv.org/abs/1712.00960|FSSD: Feature Fusion Single Shot Multibox Detector||
||工場|sakurai|https://ieeexplore.ieee.org/document/7864335|A Generic Deep-Learning-Based Approach for Automated Surface Inspection||
|3|半教師|@antimon2|https://arxiv.org/abs/1805.09302|Input and Weight Space Smoothing for Semi-supervised Learning||
||Pruning|@cohama|https://arxiv.org/abs/1805.11394|A novel channel pruning method for deep neural network compression||
||GAN 顔|@yunishimura|https://www.jstage.jst.go.jp/articleke/advpub/0/advpub_TJSKE-D-17-00085/_pdf/-char/ja|Face Image Generation System using Attributes Information with DCGANs|
||学習率スケジューリング|@wkluk-hk|https://arxiv.org/abs/1506.01186v6|Cyclical Learning Rates for Training Neural Networks||
||工場|sakurai|http://journals.sagepub.com/doi/pdf/10.1177/1687814018766682|Intelligent defect classification system based on deep learning||
||3D SLAM|@melleo1978|https://arxiv.org/abs/1803.02286|Learning monocular visual odometry with dense 3D mapping from dense 3D flow||
||強化学習|@kmiwa|https://arxiv.org/abs/1804.00379|Recall Traces: Backtracking Models for Efficient Reinforcement Learning||
||GAN attention|@n-kats|https://arxiv.org/abs/1805.0831|Self-Attention Generative Adversarial Networks|SAGAN||
||自然言語 文書埋め込み|@nharu1san|https://arxiv.org/abs/1803.11175|Universal Sentence Encoder||
|4|トラッキング MOT|@wkluk-hk|https://arxiv.org/abs/1711.02741|Recurrent Autoregressive Networks for Online Multi-Object Tracking|
||GAN|takagi|https://arxiv.org/abs/1702.08431|Boundary-Seeking Generative Adversarial Networks|
||ライブラリ|@cohama|https://arxiv.org/abs/1410.0759|cuDNN: Efficient Primitives for Deep Learning|cuDNN|
||キャプション生成|@Denpa92|https://arxiv.org/abs/1806.04510|Dank Learning: Generating Memes Using Deep Neural Networks|
||自然言語 構文|@nharu1san|http://www.aclweb.org/anthology/P15-1162|Deep Unordered Composition Rivals Syntactic Methods for Text Classification|
||強化学習 関係|@kmiwa|https://arxiv.org/abs/1806.01830|Relational Deep Reinforcement Learning|
||強化学習|@shuuichi|https://arxiv.org/abs/1803.07055|Simple random search provides a competitive approachto reinforcement learning||
||強化学習|@antimon2|https://arxiv.org/abs/1806.00175|Strategic Object Oriented Reinforcement Learning||
||転移学習|sakurai|https://ieeexplore.ieee.org/document/7966162|Transfer Learning for Automated Optical Inspection||
||GAN 音|@hissanova|https://arxiv.org/abs/1802.04208|Synthesizing Audio with Generative Adversarial Networks||
||GAN|@n-kats|https://arxiv.org/abs/1709.01118|WESPE: Weakly Supervised Photo Enhancer for Digital Cameras||
|6|インスタンスセグメンテーション|@wkluk-hk|https://arxiv.org/abs/1807.05361|Non-local RoIs for Instance Segmentation||
||フロー系生成モデル|@antimon2|https://arxiv.org/abs/1807.03039|Glow: Generative Flow with Invertible 1×1 Convolutions|Glow|
||強化学習|@kmiwa|https://arxiv.org/abs/1802.03006|Learning and Querying Fast Generative Models for Reinforcement Learnin||
||ライブラリ|@kencyke|http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform|TFX: A TensorFlow-Based Production-Scale Machine Learning Platform|TFX|
||NN一般 初期化|@melleo1978|https://arxiv.org/abs/1702.08591|The Shattered Gradients Problem: If resnets are the answer, then what is the question?||
||画像データセット|@yuji38kwmt|https://www.arxiv-vanity.com/papers/1805.04687/|BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling|BDD100K
||グラフ系|@n-kats|https://arxiv.org/abs/1806.01261|Relational inductive biases, deep learning, and graph networks|GN|
||強化学習|@kmiwa|https://arxiv.org/pdf/1707.06203|Imagination-Augmented Agents for Deep Reinforcement Learning|
||自然言語データセット|@nharu1san|https://arxiv.org/abs/1312.3005|One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling||
||転移学習|sakurai|https://arxiv.org/abs/1805.08974|Do Better ImageNet Models Transfer Better ?||
|7|対話|takagi|https://www.jstage.jst.go.jp/article/tjsai/33/1/33_DSH-F/_pdf|Engagement Recognition from Listener’s Behaviors in Spoken Dialogue Using a Latent Character Model||
||蒸留|yasuno|http://export.arxiv.org/pdf/1805.04770|Born Again Neural Networks||
||アノテーション|@yuji38kwmt|https://arxiv.org/abs/1809.08888|Empirical Methodology for Crowdsourcing Ground Truth||
||物体検出|@n-kats|https://arxiv.org/abs/1711.07240|MegDet: A Large Mini-Batch Object Detector|MegDet|
||軽量モデル|@cohama|https://arxiv.org/abs/1807.11164|ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design|ShuffleNet V2|
||NN一般 活性化|@antimon2|https://arxiv.org/abs/1810.01829|Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural Network||
|8|理論 NN一般 初期化|@melleo1978|https://arxiv.org/abs/1806.05393v2|Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks||
||インスタンスセグメンテーション 点群 医療|@wkluk-hk|https://arxiv.org/abs/1811.03208|Deep Semantic Instance Segmentation of Tree-like Structures Using Synthetic Data||
||ライブラリ|@antimon2|https://arxiv.org/abs/1810.09868|Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs||
||3D|@n-kats|https://arxiv.org/abs/1804.01654|Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images||
||軽量モデル|@cohama|http://openaccess.thecvf.com/content_ECCV_2018/html/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.html|SkipNet: Learning Dynamic Routing in Convolutional Networks||
||画像データセット アノテーション|@yuji38kwmt|https://arxiv.org/abs/1712.08394|The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for Traffic Vision Research||
||神経科学|@nharu1san|https://arxiv.org/abs/1811.02923|Universal Spike Classifier||
|9|量子化|@melleo1978|https://openreview.net/forum?id=ByfPDyrYim|Linear Backprop in non-linear networks||
||異常検知|@devjap|https://arxiv.org/abs/1703.05921|Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery||
||アノテーション|@yuji38kwmt|https://www.researchgate.net/publication/291249011_Crowdsourcing_annotations_for_visual_object_detection|Crowdsourcing Annotations for Visual Object Detection||
||距離 最適輸送|@n-kats|https://arxiv.org/abs/1811.02834|Fused Gromov-Wasserstein distance for structured objects: theoretical foundations and mathematical properties||
||転移学習|sakurai|https://arxiv.org/abs/1808.01974|A Survey of Deep Transfer Learning||
||NN一般 padding|@antimon2|https://arxiv.org/abs/1811.11718|Partial Convolution based Padding||
||軽量モデル NAS|@cohama|https://arxiv.org/abs/1812.02975|ShuffleNASNets: Efficient CNN models through modified Efficient Neural Architecture Search||
|10|理論 正規化|@melleo1978|https://arxiv.org/abs/1805.11604v3|How Does Batch Normalization Help Optimization?||
||対話|takagi|http://www.cs.toronto.edu/face2face|A face to face neural conversation model||
||蒸留 転移学習|sakurai|https://arxiv.org/abs/1503.02531|Distilling the Knowledge in a Neural Network||
||物体検出|@cohama|https://arxiv.org/abs/1804.06215|DetNet: A Backbone network for Object Detection||
||運転 車線|@yuji38kwmt|https://arxiv.org/abs/1806.05984||Ego-Lane Analysis System (ELAS): Dataset and Algorithms||
||ライブラリ|@antimon2|https://arxiv.org/abs/1812.09064|GaussianProcesses.jl: A Nonparametric Bayes package for the Julia Language|
||3D SLAM|@n-kats|https://arxiv.org/abs/1811.06152|Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos||
|11|GAN|@melleo1978|https://arxiv.org/abs/1812.04948|A Style-Based Generator Architecture for Generative Adversarial Networks|StyleGAN|
||軽量モデル|@cohama|https://arxiv.org/abs/1901.09615|Convolutional Neural Networks with Layer Reuse||
||NAS グラフ系|@n-kats|https://arxiv.org/abs/1808.07233|Neural Architecture Optimization||
||時系列|sakurai|https://arxiv.org/abs/1811.01533|Transfer learning for time series classification||
|12|ライブラリ|@antimon2|https://arxiv.org/abs/1902.02376|DiffEqFlux.jl — A Julia Library for Neural Differential Equations||
||自然言語 質問回答|@nharu1san|https://arxiv.org/abs/1902.01718|End-to-End Open-Domain Question Answering with BERTserini||
||軽量モデル|@cohama|https://arxiv.org/abs/1902.09701|Learning Implicitly Recurrent CNNs Through Parameter Sharing||
|13|GAN 3D|@melleo1978|https://arxiv.org/abs/1904.01326|HoloGAN: Unsupervised learning of 3D representations from natural images|HoloGAN|
||GAN|takagi|https://arxiv.org/abs/1811.10597v2|GAN Dissection: Visualizing and Understanding Generative Adversarial Networks||
||3D SLAM|@n-kats|https://arxiv.org/abs/1904.04998|Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras||
||自然言語 文書分類|@nharu1san|https://arxiv.org/abs/arxiv_1904.08398|DocBERT: BERT for Document Classification||
||物体検出|@cohama|https://arxiv.org/abs/1904.01355v3|FCOS: Fully Convolutional One-Stage Object Detection|FCOS|
||時系列|sakurai|http://www.lumenai.fr/blog/time-series-aggregation|Time series aggregation Comparison of two global averaging approaches||
||運転 車線|@yuji38kwmt|https://arxiv.org/abs/1802.05591|Towards End-to-End Lane Detection: an Instance Segmentation Approach||
|14|NN一般 初期化|@melleo1978|https://arxiv.org/abs/1901.09321|Fixup Initialization: Residual Learning Without Normalization|Fixup|
||自然言語|@nharu1san|https://arxiv.org/abs/1905.05950|BERT Rediscovers the Classical NLP Pipeline||
||物体検出|@cohama|https://arxiv.org/abs/1904.08189|CenterNet: Keypoint Triplets for Object Detection||
||運転|@yuji38kwmt|https://arxiv.org/abs/1904.08980|Exploring the Limitations of Behavior Cloning for Autonomous Driving||
||インスタンスセグメンテーション|@antimon2|https://arxiv.org/abs/1708.02551|Semantic Instance Segmentation with a Discriminative Loss Function||
||3D|@n-kats|https://arxiv.org/abs/1812.03828|Occupancy Networks: Learning 3D Reconstruction in Function Space||
||埋め込み 顔|takagi|https://arxiv.org/abs/1811.11283|A Compact Embedding for Facial Expression Similarity||
|15|半教師|@melleo1978|https://arxiv.org/abs/1904.12848v1|Unsupervised Data Augmentation||
||強化学習|@Mit-Funa|https://deepmind.com/blog/capture-the-flag-science/?utm_source=Deep+Learning+Weekly&utm_campaign=cf75ae36f6-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-cf75ae36f6-73708453|Human-level performance in 3D multiplayer games with populationbased reinforcement learning||
||半教師|@cohama|https://arxiv.org/abs/1905.02249v1|MixMatch: A Holistic Approach to Semi-Supervised Learning|MixMatch|
||自然言語 司法|@nharu1san|https://arxiv.org/abs/1906.02059|Neural Legal Judgment Prediction in English||
||理論|@n-kats|https://arxiv.org/abs/1805.08522|Deep learning generalizes because the parameter-function map is biased towards simple functions||
|16|データオーグメント|@melleo1978|https://arxiv.org/abs/1810.12890|DropBlock: A regularization method for convolutional networks||
||最適化|@Kgm1500|https://arxiv.org/abs/1802.09568|Shampoo: Preconditioned Stochastic Tensor Optimization||
||画風変換|@n-kats|https://arxiv.org/abs/1703.06868|Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization|AdaIN|
||NN一般|@cohama|https://arxiv.org/abs/1703.06211|Deformable Convolutional Networks||
||ライブラリ|@antimon2|https://estadistika.github.io//julia/python/packages/knet/flux/tensorflow/machine-learning/deep-learning/2019/06/20/Deep-Learning-Exploring-High-Level-APIs-of-Knet.jl-and-Flux.jl-in-comparison-to-Tensorflow-Keras.html|Deep Learning: Exploring High Level APIs of Knet.jl and Flux.jl in comparison to Tensorflow-Keras||
||強化学習環境|@Mit-Funa|https://ai.googleblog.com/2019/06/introducing-google-research-football.html|Google Research Football: A Novel Reinforcement Learning Environment|
|17|最適化|@melleo1978|https://arxiv.org/abs/1905.11286|Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks|NovoGrad|
||自己教師 optical_flow|@n-kats|https://arxiv.org/abs/1904.09117|SelFlow: Self-Supervised Learning of Optical Flow||
||自己教師 トラッキング|@cohama|https://arxiv.org/abs/1806.09594|Tracking Emerges by Colorizing Videos||
|18|attention|@melleo1978|https://arxiv.org/abs/1906.05909|Stand-Alone Self-Attention in Vision Models||
||物体検出|@cohama|https://arxiv.org/abs/1909.03625|CBNet: A Novel Composite Backbone Network Architecture for Object Detection||
||グラフ系 物理|@n-kats|https://arxiv.org/abs/1909.02487|Ab-Initio Solution of the Many-Electron Schrödinger Equation with Deep Neural Networks|FermiNet|
||ライブラリ|@antimon2|https://dl.acm.org/inst_page.cfm?id=60022195|Gen: A General-Purpose Probabilistic Programming System with Programmable Inference||
||グラフ系 物理|@yuji38kwmt|https://arxiv.org/abs/1906.10033|Unifying machine learning and quantum chemistry – a deep neural network for molecular wavefunctions||
|19|tutorial|@FunabikiKeisuke|https://arxiv.org/abs/1909.13739|https://www.programiz.com/python-programming|
||フロー系生成 グラフ系 物理|@n-kats|https://arxiv.org/abs/1909.13739|Equivariant Hamiltonian Flows||
|20|contrastive_learning|@melleo1978|https://arxiv.org/abs/1911.05722v2|Momentum Contrast for Unsupervised Visual Representation Learning|MoCo|
||異常検知|@wkluk-hk|https://arxiv.org/abs/1802.06222|Efficient GAN-Based Anomaly Detection||
||自然言語|@nharu1san|https://arxiv.org/abs/1910.13461|BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension||
||動画生成 姿勢|@n-kats|https://arxiv.org/abs/1910.12713|Few-shot Video-to-Video Synthesis||
||アノテーション|@yuji38kwmt|https://arxiv.org/abs/1911.02807|Improving Human Annotation in Single Object Tracking||
||物体検出|@cohama|https://arxiv.org/abs/1901.01892|Scale-Aware Trident Networks for Object Detection||
|21|GAN|@melleo1978|https://arxiv.org/abs/1912.04958|Analyzing and Improving the Image Quality of StyleGAN|StyleGAN2|
||NAS|@wkluk-hk|https://arxiv.org/abs/1807.11626|MnasNet: Platform-Aware Neural Architecture Search for Mobile|MNAS|
||強化学習|@Mit-Funa|https://arxiv.org/abs/1912.00167|IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks||
||検索|@yuji38kwmt|https://arxiv.org/abs/1903.04638|Challenges in Search on Streaming Services: Netflix Case Study||
||NN一般|@n-kats|https://arxiv.org/abs/1905.11786|Putting An End to End-to-End: Gradient-Isolated Learning of Representations|GIM|
||ライブラリ SLAM|@nharu1san|https://arxiv.org/abs/1910.01122|OpenVSLAM: A Versatile Visual SLAM Framework|OpenVSLAM|
||ライブラリ|@cohama|https://www.atmarkit.co.jp/ait/articles/1910/31/news028.html|PyTorch vs. TensorFlow||
|22|画像データセット アノテーション 運転 キャプション|@melleo1978|https://arxiv.org/abs/1807.11546|Textual Explanations for Self-Driving Vehicles|BDD-X|
||データ 転移学習|@wkluk-hk|https://arxiv.org/abs/2001.02799|Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data||
||3D|@antimon2|https://arxiv.org/abs/2001.05422|Indoor Layout Estimation by 2D LiDAR and Camera Fusion||
||強化学習|@n-kats|https://arxiv.org/abs/1911.08265|Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model|MuZero|
||ライブラリ エッジ|@Mit-Funa|https://github.com/NNgen/nngen|NNgen||
|23|contrastive_learning|@melleo1978|https://arxiv.org/abs/2002.05709|A Simple Framework for Contrastive Learning of Visual Representations|SimCLR|
||セグメンテーション|@FunabikiKeisuke|http://mprg.jp/data/MPRG/F_group/F20191205_goto.pdf|カメラ間の整合性を考慮した全周囲画像のセグメンテーション||
||強化学習|@Mit-Funa|https://arxiv.org/abs/1806.06923|Implicit Quantile Networks for Distributional Reinforcement Learning||
||アノテーション|@yuji38kwmt|https://arxiv.org/abs/2002.06626|Block Annotation: Better Image Annotation for Semantic Segmentation with Sub-Image Decomposition|
||理論 蒸留|@n-kats|https://arxiv.org/abs/2002.05715|Self-Distillation Amplifies Regularization in Hilbert Space||
||物体検出|@cohama|https://arxiv.org/abs/1911.12451v3|Empirical Upper Bound in Object Detection and More||
|24|3D SLAM|@melleo1978|https://arxiv.org/abs/2002.05709|Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion from 3D Geometry||
||強化学習 world_model|@Mit-Funa|https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html?m=1|Introducing Dreamer: Scalable Reinforcement Learning Using World Models||
||物体検出 レア事例|@cohama|https://arxiv.org/abs/2003.05176|Equalization Loss for Long-Tailed Object Recognition||
||NN一般 正規化|@n-kats|https://arxiv.org/abs/2002.10444|Batch Normalization Biases Deep Residual Networks Towards Shallow Paths|Skip init|
|25|3D|@strshp|https://drive.google.com/file/d/17ki_YAL1k5CaHHP3pIBFWvw-ztF4CCPP/view|3D Photography using Context-aware Layered Depth Inpainting||
||高速化|@melleo1978|https://arxiv.org/abs/1903.03129v2|SLIDE : IN DEFENSE OF SMART ALGORITHMS OVER HARDWARE ACCELERATION FOR LARGE-SCALE DEEP LEARNING SYSTEMS|SLIDE|
||VAE|@sennin0901|https://arxiv.org/abs/1906.00446|Generating Diverse High-Fidelity Images with VQ-VAE-2|VQ-VAE-2|
||3D グラフ系 点群 インスタンスセグメンテーション|@n-kats|https://arxiv.org/abs/2003.13867|3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation||
||NN一般 軽量モデル|@cohama|https://arxiv.org/abs/2003.13549v2|Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets|BSConv|
||アノテーション|@usakotail|https://www.arxiv-vanity.com/papers/1909.12493/|Invisible Marker:Automatic Annotation for Object Manipulation||
|26|GAN 姿勢|@strshp|https://menyifang.github.io/projects/ADGAN/ADGAN_files/Paper_ADGAN_CVPR2020.pdf|Controllable Person Image Synthesis with Attribute-Decomposed GAN||
||GAN auto_encoder|@sennin0901|https://arxiv.org/abs/2004.04467|Adversarial Latent Autoencoders||
||最適化|@melleo1978|https://arxiv.org/abs/2001.06782v2|Gradient Surgery for Multi-Task Learning||
||contrastive_learning world_model|@n-kats|https://arxiv.org/abs/1911.12247|Contrastive Learning of Structured World Models|C-SWM|
||NN一般 正規化|@cohama|https://arxiv.org/abs/1905.11926v4|Network Deconvolution||
||物体検出|@usakotail|https://arxiv.org/abs/2004.10934|YOLOv4: Optimal Speed and Accuracy of Object Detection|YOLOv4|
|27|物体検出|@n-kats|https://arxiv.org/abs/2005.12872|End-to-End Object Detection with Transformers|DETR|
||物体検出|@cohama|https://arxiv.org/abs/2006.04388v1|Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection|Gemeralized fOcal loss|
||因果推論|@K_Ryuichirou|https://www.pnas.org/content/116/10/4156|Metalearners for estimating heterogeneous treatment effects using machine learning||
||GAN ファッション|@usakotail|https://openaccess.thecvf.com/content_CVPR_2020/html/Neuberger_Image_Based_Virtual_Try-On_Network_From_Unpaired_Data_CVPR_2020_paper.html|Image Based Virtual Try-on Network from Unpaired Data||
||contrastive_learning 自己教師|@melleo1978|https://arxiv.org/abs/2006.07733v1|Bootstrap Your Own Latent A New Approach to Self-Supervised Learning|BYOL|
|28|教師ノイズ|@cohama|https://arxiv.org/abs/1910.00701|Distilling Effective Supervision from Severe Label Noise||
||自然言語|@n-kats|https://arxiv.org/abs/2006.16236|Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention||
||点群|@usakotail|https://arxiv.org/abs/1612.00593|PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation|PointNet|
||contrastive_learning 自己教師|@melleo1978|https://arxiv.org/abs/2005.04966v2|Prototypical Contrastive Learning of Unsupervised Representations||
||インスタンスセグメンテーション トラッキング|@sennin0901|https://arxiv.org/abs/1912.04573|Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation||
|29|強化学習|@Mit-Funa|https://arxiv.org/abs/1909.01387|Making Efficient Use ofDemonstrations to Solve Hard Exploration Problems|R2D3|
||3D|@n-kats|https://arxiv.org/abs/1909.01387|NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections|NeRF-W|
||アノテーション インスタンスセグメンテーション|@strshp|https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/495_ECCV_2020_paper.php|PhraseClick: Toward Achieving Flexible Interactive Segmentation by Phrase and Click|PhraseClick|
||画像分類|@cohama|https://arxiv.org/abs/2007.09558|Resolution Switchable Networks for Runtime Efficient Image Recognition||
||3D|@melleo1978|https://arxiv.org/abs/2003.10432v2|Atlas: End-to-End 3D Scene Reconstruction from Posed Images|Atlas|
|30|contrastive_learning 自己教師|@cohama|https://arxiv.org/abs/2008.00261|Distilling Visual Priors from Self-Supervised Learning||
||3D|@n-kats|https://arxiv.org/abs/2008.09497|Single-Image Depth Prediction Makes Feature Matching Easier||
|31|物体検出|@usako_tail|https://arxiv.org/abs/1812.05784|PointPillars: Fast Encoders for Object Detection from Point Clouds||
||強化学習|@n-kats|https://arxiv.org/abs/2007.13732|Learning Lane Graph Representations for Motion Forecasting||
||強化学習||https://arxiv.org/pdf/2005.05960.pdf|どんなもの？||
||生成/GAN|@strshp|https://arxiv.org/pdf/2008.07783.pdf|Mesh Guided One-shot Face Reenactment Using Graph Convolutional Networks||
|32|NN一般|@n-kats|https://arxiv.org/abs/2006.13473|AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types||
||強化学習||https://arxiv.org/pdf/1710.01457.pdf|Learning to Segment Human by Watching YouTube||
|33|物体検出|@cohama|https://arxiv.org/abs/2010.04159|どんなもの?|DETR|
||生成/GAN|@n-kats|https://arxiv.org/abs/2010.15040|Training Generative Adversarial Networks by Solving Ordinary Differential Equations||
|34|物体検出|@n-kats|https://arxiv.org/abs/2101.02702|TrackFormer: Multi-Object Tracking with Transformers|DETR|
|35|物体検出|@n-kats|https://arxiv.org/abs/2012.09688|PCT: Point Cloud Transformer|DETR|
|36|再識別 Transformer|@n-kats|https://arxiv.org/abs/2102.04378|TransReID: Transformer-based Object Re-Identification|TransReID|
|37|生成/GAN|@cohama|https://arxiv.org/abs/2104.04767v1|MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis|MobileStyleGAN|
||3D SLAM|@n-kats|https://arxiv.org/abs/2103.12352|iMAP: Implicit Mapping and Positioning in Real-Time||
|38|NN一般 Transformer|@cohama|https://arxiv.org/abs/2103.14030v1|Swin Transformer: Hierarchical Vision Transformer using Shifted Windows|Swin Transformer|
||物体検出|@n-kats|https://arxiv.org/abs/2104.01318|Efficient DETR: Improving End-to-End Object Detector with Dense Prior|DETR|
|39|contrastive_learning 自己教師 Survey|@ma|https://arxiv.org/abs/2011.00362v3|A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING||
||物体検出 Transformer|@n-kats|https://arxiv.org/abs/2106.00666|You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection|YOLOS|
||生成/GAN フォント Few-shot|@derwind|https://arxiv.org/abs/2104.00887|Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts||
|40|アノテーション 3D セグメンテーション|@yuji38kwmt|https://ieeexplore.ieee.org/document/9363898|Annotation Tool and Urban Dataset for 3D Point Cloud Semantic Segmentation|PC-Annotate|
||セグメンテーション Transformer|@cohama|https://arxiv.org/abs/2107.06278v1|Per-Pixel Classification is Not All You Need for Semantic Segmentation|MaskFormer|
|46|理論 学習|@n-kats|https://arxiv.org/abs/2201.02177|Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets|Grokking|
||NN一般 ViT|@cohama|https://arxiv.org/abs/2112.13492v1|Vision Transformer for Small-Size Datasets|ViT|
||生成/GAN||https://arxiv.org/pdf/2111.01007.pdf|Projected GANs Converge Faster|Projected GAN|
||ネットワーク 輻輳制御|@yuji38kwmt|https://ieeexplore.ieee.org/abstract/document/8859212|DL-TCP: Deep Learning-Based Transmission Control Protocol for Disaster 5G mmWave Networks||
|47|3D NeRF|@n-kats|https://arxiv.org/abs/2202.05263|Block-NeRF: Scalable Large Scene Neural View Synthesis|NeRF|
||NN一般 Transformer|@cohama|https://arxiv.org/abs/2201.09450|UniFormer: Unifying Convolution and Self-attention for Visual Recognition|UniFormer|
||マルチモーダル||https://tech.fusic.co.jp/posts/2021-12-29-vilt/|ViLT 解説記事||
|48|生成 画像変換|@derwind|https://nips2017creativity.github.io/doc/ASCII_Art_Synthesis.pdf|ASCII Art Synthesis with Convolutional Networks||
||物体検出 DETR|@cohama|https://arxiv.org/abs/2203.03605v1|DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection|DETR|
|49|マルチモーダル|@n-kats|https://arxiv.org/abs/2202.10890|Hierarchical Perceiver||
||データセット 自動運転|@yuji38kwmt|https://arxiv.org/abs/2112.12610|Pandaset: Advanced Sensor Suite Dataset||
||学習 高速化 メモリ|@cohama|https://arxiv.org/abs/2203.16755|Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models|SBP|
||画像変換 スケッチ|@masahiro6510|https://esslab.jp/~ess/publications/SimoSerraSIGGRAPH2016.pdf|Learning to Simplify Fully Convolutional Networks for Rough Sketch Cleanup||
|50|点群 表現|@cohama|https://arxiv.org/abs/2205.05740v2|Surface Representation for Point Clouds|RepSurf|
||サーベイ 自然言語 KB|@n-kats|https://arxiv.org/abs/2204.06031|A Review on Language Models as Knowledge Bases||
||理論 可視化||https://arxiv.org/abs/1712.09913|Visualizing the Loss Landscape of Neural Nets||
|51|点群|@cohama|https://arxiv.org/abs/2206.04670v1|PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies|PointNeXt|
||数式認識 OCR|@n-kats|https://arxiv.org/abs/2203.01601|Syntax-Aware Network for Handwritten Mathematical Expression Recognition|SAN|
|52|動画 Transformer|@n-kats|https://arxiv.org/abs/2102.05095|Is Space-Time Attention All You Need for Video Understanding? (TimeSformer)|TimeSformer|
||点群|@cohama|https://arxiv.org/abs/2202.07123|Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework|PointMLP|
|53|生成/編集 Diffusion|@n-kats|https://arxiv.org/abs/2208.01626|Prompt-to-Prompt Image Editing with Cross Attention Control|Prompt-to-Prompt|
||量子化 学習|@cohama|https://arxiv.org/abs/2207.08822v1|Is Integer Arithmetic Enough for Deep Learning Training?|int8|
|54|量子化 FP8|@cohama|https://arxiv.org/abs/2206.02915|FP8 Formats for Deep Learning|FP8|
||生成 Diffusion|@n-kats|https://arxiv.org/abs/2112.10752|High-Resolution Image Synthesis with Latent Diffusion Models|Latent Diffusion|
|55|NN一般 アテンション|@n-kats|https://arxiv.org/abs/2209.15001|Dilated Neighborhood Attention Transformer|DiNA|
||ViT 高速化|@cohama|https://arxiv.org/abs/2210.09461v1|Token Merging: Your ViT But Faster|ToMe|
|56|セグメンテーション 統一|@n-kats|https://arxiv.org/abs/2211.06220|OneFormer: One Transformer to Rule Universal Image Segmentation|OneFormer|
||CNN MetaFormer|@cohama|https://arxiv.org/abs/2211.05778v2|InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions|InternImage|
||プルーニング|@yuji38kwmt|https://arxiv.org/abs/1707.06168|Channel Pruning for Accelerating Very Deep Neural Networks||
|57|トラッキング|@cohama|https://arxiv.org/abs/2211.09791v1|MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors|MOTRv2|
||3D 物体検出 点群|@n-kats|https://arxiv.org/abs/2210.07372|SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds|SWFormer|
|58|BCI グラフ|@n-kats|https://arxiv.org/abs/2105.02786v3|LGGNet: Learning from Local-Global-Graph Representations for Brain-Computer Interface|LGGNet|
||生成 Diffusion|@cohama|https://arxiv.org/abs/2010.02502|Denoising Diffusion Implicit Models|DDIM|
||推薦 バイアス|@morinota|https://arxiv.org/pdf/1901.07555.pdf|Managing Popularity Bias in Recommender Systems with Personalized Re-ranking||
|59|LLM ツール使用|@n-kats|https://arxiv.org/abs/2302.04761|Toolformer: Language Models Can Teach Themselves to Use Tools|Toolformer|
||推薦 バイアス|@morinota|https://dl.acm.org/doi/fullHtml/10.1145/3523227.3546757|Countering Popularity Bias by Regularizing Score Differences||
||生成 Diffusion|@cohama|https://arxiv.org/abs/2301.11093v1|Simple Diffusion: End-to-End Diffusion for Image Synthesis|Simple Diffusion|
|60|動画 ViT|@n-kats|https://arxiv.org/abs/2212.03229|Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning|TubeViT|
|41|検索 画像検索|@strshp|https://arxiv.org/pdf/2008.07783.pdf|Compositional Sketch Search||
||マルチモーダル|@n-kats|https://arxiv.org/abs/2107.14795|Perceiver IO: A General Architecture for Structured Inputs & Outputs|Perceiver|
||物体検出 半教師あり|@cohama|https://arxiv.org/abs/2106.09018v3|End-to-End Semi-Supervised Object Detection with Soft Teacher||
||物体検出|@usako_tail|https://arxiv.org/abs/2107.08430|YOLOX: Exceeding YOLO Series in 2021|YOLOX|
|42|3D NeRF|@n-kats|https://arxiv.org/abs/2103.10380|FastNeRF: High-Fidelity Neural Rendering at 200FPS|NeRF|
||LLM Code||https://arxiv.org/abs/2107.03374|Evaluating Large Language Models Trained on Code|Codex|
||金融|@kmdqcom|https://arxiv.org/pdf/2012.07245.pdf|Deep Portfolio Optimization via Distributional Prediction of Residual Factors||
||高速化 行列|@cohama|https://arxiv.org/abs/2106.10860|Multiplying Matrices Without Multiplying|MADNESS|
|43|応用 物理 物体検出|@jyoshida_sci|https://doi.org/10.1038/s42254-021-00371-w|New directions in hypernuclear physics||
||3D レジストレーション|@yuji38kwmt|https://arxiv.org/abs/2104.03501|DeepI2P: Image-to-Point Cloud Registration via Deep Classification||
||画像分類 学習|@cohama|https://arxiv.org/abs/2110.00476v1|ResNet strikes back: An improved training procedure in timm|ResNet|
||理論 位置埋め込み|@n-kats|https://arxiv.org/abs/2107.02561|Rethinking Positional Encoding||
||生成/GAN フォント|@derwind|https://arxiv.org/abs/2005.10510|Few-shot Compositional Font Generation with Dual Memory||
|44|自己教師 ViT|@cohama|https://arxiv.org/abs/2111.06377v1|Masked Autoencoders Are Scalable Vision Learners|MAE|
||画像分類 アーキテクチャ||https://arxiv.org/abs/2106.04803v2|CoAtNet: Marrying Convolution and Attention for All Data Sizes|CoAtNet|
||セグメンテーション 教師なし|@n-kats|https://arxiv.org/abs/2111.06349|Unsupervised Part Discovery from Contrastive Reconstruction||
|45|ViT 高速化|@cohama|https://arxiv.org/abs/2112.07658v1|AdaViT: Adaptive Tokens for Efficient Vision Transformer|AdaViT|
||強化学習 ネットワーク|@yuji38kwmt|https://ieeexplore.ieee.org/document/8357943|QTCP: Adaptive Congestion Control with Reinforcement Learning||
||理論 ロバスト性|@n-kats|https://arxiv.org/abs/2105.12806v3|A Universal Law of Robustness via Isoperimetry||
||生成/GAN 文字|@derwind|https://github.com/kaonashi-tyc/zi2zi|zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks|zi2zi|
|61|LLM 理論|@n-kats|https://arxiv.org/abs/2304.04498|Towards Digital Nature: Bridging the Gap between Turing Machine Objects and Linguistic Objects in LLMMs for Universal Interaction of Object-Oriented Descriptions|LLMM|
|62|NN一般 活性化||https://arxiv.org/abs/1412.6830|LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS|APL|
|63|LLM ツール使用|@n-kats|https://arxiv.org/abs/2305.17126|Large Language Models as Tool Makers|ToolMaker|
|64|LLM 強化学習 エージェント|@n-kats|https://arxiv.org/abs/2303.11366v3|Reflexion: Language Agents with Verbal Reinforcement Learning|Reflexion|
|65|自然言語 Transformer|@masahiro6510|https://arxiv.org/abs/1706.03762|Attention Is All You Need|Transformer|
|66|LLM 推論|@n-kats|https://arxiv.org/abs/2308.09687|Graph of Thoughts: Solving Elaborate Problems with Large Language Models|GoT|
|67|生成 世界モデル 自動運転|@wakodai|https://arxiv.org/abs/2309.17080|GAIA-1: A Generative World Model for Autonomous Driving|GAIA-1|
|68|LLM ツール使用 強化学習|@n-kats|https://arxiv.org/abs/2310.12931|Eureka: Human-Level Reward Design via Coding Large Language Models|Eureka|
|69|LLM 自己学習 評価|@n-kats|https://arxiv.org/abs/2312.06585|Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models|ReST-EM|
|70|LLM ベンチマーク 評価|@n-kats|https://arxiv.org/abs/2311.12022|GPQA: A Graduate-Level Google-Proof Q&A Benchmark|GPQA|
|71|LLM ICL|@n-kats|https://arxiv.org/abs/2402.04248|Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks|Mamba|
|72|LLM 評価 倫理|@n-kats|https://arxiv.org/abs/2403.11152|Evaluation Ethics of LLMs in Legal Domain|LLM Ethics|
|73|マルチモーダル 評価 LVLM|@n-kats|https://arxiv.org/abs/2403.20330|Are We on the Right Way for Evaluating Large Vision-Language Models?|MMStar|
|74|マルチモーダル VLM|@n-kats|https://arxiv.org/abs/2405.02246|What matters when building vision-language models?|Idefics2|
|75|物体検出 YOLO|@masahiro6510|https://arxiv.org/abs/2405.14458|YOLOv10: Real-Time End-to-End Object Detection|YOLOv10|
|76|LLM RAG|@n-kats|https://arxiv.org/abs/2404.16130|From Local to Global: A Graph RAG Approach to Query-Focused Summarization|GraphRAG|
|77|LLM 評価 推論|@n-kats|https://arxiv.org/abs/2401.06416|Mission: Impossible Language Models|ImpossibleLM|
|78|マルチモーダル 統一|@cohama|http://arxiv.org/abs/2408.12528|Show-o: One Single Transformer to Unify Multimodal Understanding and Generation|Show-o|
|79|セグメンテーション 教師なし|@n-kats|https://arxiv.org/abs/2406.20081|Segment Anything without Supervision|UnSAM|
|80|LLM 構造化推論 自動化|@n-kats|https://arxiv.org/abs/2411.03562|Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level|AgentK|
|81|LLM 計画 ゲーム|@n-kats|https://arxiv.org/abs/2412.12119|Mastering Board Games by External and Internal Planning with Language Models|MAV|
|82|自然言語 トピックモデル|@n-kats|https://arxiv.org/abs/2203.05794|BERTopic: Neural topic modeling with a class-based TF-IDF procedure|BERTopic|
|83|LLM 推論 強化学習|@n-kats|https://arxiv.org/abs/2501.12948|DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning|DeepSeek-R1|
|84|LLM メモリ 対話|@n-kats|https://arxiv.org/abs/2503.08026|In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents|RMM|
|85|ロボティクス マルチモーダル|@n-kats|https://arxiv.org/abs/2503.20020|Gemini Robotics: Bringing AI into the Physical World|Gemini Robotics|
|86|LLM RAG マルチモーダル|@n-kats|https://arxiv.org/abs/2504.20734|UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities|UniversalRAG|
|87|LLM 推論 評価|@n-kats|https://arxiv.org/abs/2506.06941|The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity|Illusion of Thinking|
|88|LLM 科学 マルチエージェント|@n-kats|https://arxiv.org/abs/2507.04053|TopoMAS: Large Language Model Driven Topological Materials Multiagent System|TopoMAS|
